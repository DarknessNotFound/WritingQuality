{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from math import floor, ceil, sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time as time\n",
    "\n",
    "totaltime1 = time.time()\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "from os.path import exists as DoesPathExist\n",
    "train_logs_path = \"/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\"\n",
    "train_scores_path = \"/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\"\n",
    "test_logs_path = \"/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\"\n",
    "sample_submission_path = \"/kaggle/input/linking-writing-processes-to-writing-quality/sample_submission.csv\"\n",
    "\n",
    "if DoesPathExist(train_logs_path):\n",
    "    train_logs: pd.DataFrame = pd.read_csv(train_logs_path)\n",
    "else:\n",
    "    train_logs: pd.DataFrame = pd.read_csv(\"./data/train_logs.csv\")\n",
    "\n",
    "if DoesPathExist(train_scores_path):\n",
    "    train_scores = pd.read_csv(train_scores_path)\n",
    "else:\n",
    "    train_scores = pd.read_csv(\"./data/train_scores.csv\")\n",
    "\n",
    "if DoesPathExist(test_logs_path):\n",
    "    test_logs = pd.read_csv(test_logs_path)\n",
    "else:\n",
    "    test_logs = pd.read_csv(\"./data/test_logs.csv\")\n",
    "    \n",
    "if DoesPathExist(sample_submission_path):\n",
    "    sample_submission = pd.read_csv(sample_submission_path)\n",
    "else:\n",
    "    sample_submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "    \n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCUATION = ['!', '\"', \"'\", '(', ')', ',', '.', ':', ';','?', '¡', '¿', '\\\\']\n",
    "MAX_EVENT_START_TIME = 1_800_000 # miliseconds in 30 minutes.\n",
    "NUMBER_OF_INTERVALS = 1_800 # = 1_800_000 msec / 250 milisecond intervals\n",
    "\n",
    "NUM_FEATURES = 14\n",
    "NUM_FEATURES_2 = 54\n",
    "FINAL_LAYER_COUNT = 12\n",
    "\n",
    "EVENT_MAX = 6000\n",
    "WORD_MAX = 800\n",
    "PASTE_COUNT_DIVISOR = 10\n",
    "REPLACE_COUNT_DIVISOR = 10\n",
    "\n",
    "def ScoreOneHot(score: float) -> np.array:\n",
    "    result = np.zeros(12)\n",
    "    result[int(score / 0.5) - 1] = 1\n",
    "    return result\n",
    "\n",
    "def TrainingPrep(df: pd.DataFrame):\n",
    "\n",
    "    indexs_to_drop = df[df[\"down_time\"] > df.down_time.quantile(0.999)].index\n",
    "    df.drop(indexs_to_drop, inplace=True)\n",
    "    indexs_to_drop = df[df[\"up_time\"] > df.down_time.quantile(0.999)].index\n",
    "    df.drop(indexs_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def DfToX(df: pd.DataFrame, minPositiveExamples: int = 0) -> np.array:\n",
    "    indexs_to_drop = df[df[\"action_time\"] > 250].index\n",
    "    df = df.drop(indexs_to_drop, axis=0)\n",
    "    NumUniqueIds = len(np.unique(df[\"id\"]))\n",
    "\n",
    "    x_train_1 = np.zeros((NumUniqueIds, NUM_FEATURES, int(NUMBER_OF_INTERVALS)))\n",
    "    x_train_2 = np.zeros((NumUniqueIds, NUM_FEATURES_2))\n",
    "    \n",
    "    groups = df.groupby(\"id\")\n",
    "\n",
    "    for index, id in enumerate(np.unique(df[\"id\"])):\n",
    "        essayGroup = groups.get_group(id)\n",
    "        max = essayGroup[\"down_time\"].quantile(0.99)\n",
    "        min = essayGroup[\"down_time\"].quantile(0.01)\n",
    "        dt_range = max - min\n",
    "        if dt_range == 0:\n",
    "            dt_range = 1\n",
    "        if dt_range is None:\n",
    "            dt_range = 1\n",
    "\n",
    "        MAX_CURSOR_POSITION = essayGroup[\"cursor_position\"].max()\n",
    "\n",
    "        if MAX_CURSOR_POSITION is None:\n",
    "            print(\"MAX_CURSOR_POSITION is None\")\n",
    "\n",
    "        ValueActivityCounts = essayGroup.activity.value_counts().reset_index()\n",
    "        ValueEventCounts = essayGroup.down_event.value_counts().reset_index()\n",
    "        ValueTextChangeCounts = essayGroup.text_change.value_counts().reset_index()\n",
    "\n",
    "        SpaceStats = essayGroup[[\"id\", \"down_event\", \"event_id\"]]\n",
    "        SpaceStats = SpaceStats[SpaceStats[\"down_event\"].isin([\"Space\"])]\n",
    "        SpaceStats[\"word_size\"] = SpaceStats[\"event_id\"].diff()\n",
    "        SpaceStats = SpaceStats.dropna(axis=0).reset_index()\n",
    "\n",
    "        x_train_2[index][0] = essayGroup.word_count.max() / WORD_MAX\n",
    "        x_train_2[index][1] = essayGroup.event_id.max() / EVENT_MAX\n",
    "        x_train_2[index][2] = essayGroup.down_time.min() / 60_000\n",
    "\n",
    "        x_train_2[index][3] = essayGroup.activity.count() / 6000\n",
    "        x_train_2[index][4] = ValueActivityCounts[ValueActivityCounts[\"activity\"] == \"Input\"][\"count\"].max() / 6000\n",
    "        x_train_2[index][5] = ValueActivityCounts[ValueActivityCounts[\"activity\"] == \"Remove/Cut\"][\"count\"].max() / 700\n",
    "        x_train_2[index][6] = ValueActivityCounts[ValueActivityCounts[\"activity\"] == \"Paste\"][\"count\"].max() / PASTE_COUNT_DIVISOR\n",
    "        x_train_2[index][7] = ValueActivityCounts[ValueActivityCounts[\"activity\"] == \"Replace\"][\"count\"].max() / REPLACE_COUNT_DIVISOR\n",
    "\n",
    "        x_train_2[index][8] = ((x_train_2[index][7] * REPLACE_COUNT_DIVISOR) + (x_train_2[index][6] * PASTE_COUNT_DIVISOR)) / (PASTE_COUNT_DIVISOR + REPLACE_COUNT_DIVISOR)\n",
    "        \n",
    "        x_train_2[index][9] = ValueEventCounts[ValueEventCounts[\"down_event\"] == \"Shift\"][\"count\"].max() / 300\n",
    "        x_train_2[index][10] = ValueEventCounts[ValueEventCounts[\"down_event\"].isin(PUNCUATION)][\"count\"].sum() / 100\n",
    "        x_train_2[index][11] = ValueEventCounts[ValueEventCounts[\"down_event\"].isin([\"Space\", \"CapsLock\"])][\"count\"].sum() / 1000\n",
    "        x_train_2[index][12] = ValueEventCounts[ValueEventCounts[\"down_event\"].isin([\"Leftclick\", \"ArrowUp\", \"ArrowDown\", \"ArrowLeft\", \"ArrowRight\"])][\"count\"].sum() / 250\n",
    "        x_train_2[index][13] = ValueEventCounts[ValueEventCounts[\"down_event\"].isin([\"?\", \"!\"])][\"count\"].sum() / 10\n",
    "\n",
    "        x_train_2[index][14] = essayGroup.action_time.mean() / 100\n",
    "        x_train_2[index][15] = essayGroup.action_time.sum() / 500_000\n",
    "        x_train_2[index][16] = essayGroup.action_time.std() / 100\n",
    "        x_train_2[index][17] = essayGroup.action_time.skew() / 20\n",
    "\n",
    "        x_train_2[index][18] = essayGroup.down_time.mean() / 100_000\n",
    "        x_train_2[index][19] = essayGroup.down_time.sum() / 100_000_000\n",
    "        x_train_2[index][20] = essayGroup.down_time.std() / 500_000\n",
    "        x_train_2[index][21] = essayGroup.down_time.skew() / 20\n",
    "        x_train_2[index][22] = essayGroup.down_time.sem() / 10_000\n",
    "\n",
    "        x_train_2[index][23] = essayGroup.word_count.std() / 200\n",
    "        x_train_2[index][24] = essayGroup.word_count.sem() / 3\n",
    "        x_train_2[index][25] = essayGroup.word_count.mean() / 350\n",
    "\n",
    "        for i in range(10):\n",
    "            if i == 0 or i == 1:\n",
    "                x_train_2[index][26+i] = essayGroup[essayGroup.action_time >= (i+1)*50].action_time.count() / 2500\n",
    "            elif i == 2 or i == 3:\n",
    "                x_train_2[index][26+i] = essayGroup[essayGroup.action_time >= (i+1)*50].action_time.count() / 500\n",
    "            else:\n",
    "                x_train_2[index][26+i] = essayGroup[essayGroup.action_time >= (i+1)*50].action_time.count() / 25\n",
    "\n",
    "        x_train_2[index][36] = ValueEventCounts[ValueEventCounts[\"down_event\"].isin([\"AudioVolumeDown\", \"AudioVolumeMute\", \"AudioVolumeUp\", \"MediaPlayPause\", \"MediaTrackNext\", \"MediaTrackPrevious\"])][\"count\"].sum() / 10\n",
    "        x_train_2[index][37] = ValueTextChangeCounts[ValueTextChangeCounts[\"text_change\"].isin([\"q\"])][\"count\"].sum() / 1000\n",
    "        x_train_2[index][38] = ValueTextChangeCounts[ValueTextChangeCounts[\"text_change\"].isin([\" \", \"\\n\"])][\"count\"].sum() / 1000\n",
    "        x_train_2[index][39] = ValueTextChangeCounts[ValueTextChangeCounts[\"text_change\"].isin([\"NoChange\"])][\"count\"].sum() / 500\n",
    "        x_train_2[index][40] = ValueTextChangeCounts[ValueTextChangeCounts[\"text_change\"].isin([\"NoChange\", \" \", \"\\n\", \"q\"])][\"count\"].sum() / 500\n",
    "        x_train_2[index][41] = np.sum(list(map(lambda x: x.count(\"=>\"), essayGroup['text_change'])))\n",
    "\n",
    "        temp_diff = essayGroup.groupby(\"id\")[\"down_time\"].diff(periods=1, axis=0)   \n",
    "        x_train_2[index][42] = essayGroup[temp_diff > 100].dropna().groupby(\"id\")[\"event_id\"].count().max() / 4000\n",
    "        x_train_2[index][43] = essayGroup[temp_diff > 1_000].dropna().groupby(\"id\")[\"event_id\"].count().max() / 200\n",
    "        x_train_2[index][44] = essayGroup[temp_diff > 10_000].dropna().groupby(\"id\")[\"event_id\"].count().max() / 20\n",
    "        x_train_2[index][45] = essayGroup[temp_diff > 100_000].dropna().groupby(\"id\")[\"event_id\"].count().max() / 2\n",
    "        del temp_diff\n",
    "\n",
    "        x_train_2[index][46] = SpaceStats.word_size.mean() / 10\n",
    "        x_train_2[index][47] = SpaceStats.word_size.sum() / 6000\n",
    "        x_train_2[index][48] = SpaceStats.word_size.skew() / 10\n",
    "        x_train_2[index][49] = SpaceStats.word_size.std() / 15\n",
    "        x_train_2[index][50] = SpaceStats.word_size.sem() # / 1\n",
    "\n",
    "        x_train_2[index][51] = np.sum(list(map(lambda x: x.count(\"Move From\"), essayGroup['activity'])))\n",
    "        x_train_2[index][52] = ValueActivityCounts[ValueActivityCounts[\"activity\"] == \"Nonproduction\"][\"count\"].max() / 500\n",
    "        x_train_2[index][53] = (ValueActivityCounts[ValueActivityCounts['activity'].isin(['Input', 'Remove/Cut'])][\"count\"].sum() /\n",
    "                               (essayGroup.up_time.max() - essayGroup.down_time.min())) * 100\n",
    "\n",
    "        for i in range(len(x_train_2[index])):\n",
    "            if np.isnan(x_train_2[index][i]):\n",
    "                x_train_2[index][i] = 0\n",
    "\n",
    "        # Build X Train 1 (convolution)\n",
    "        for row in essayGroup.itertuples():\n",
    "\n",
    "            if (row.down_time < min*0.9) or (row.down_time > max*1.1):\n",
    "                continue\n",
    "\n",
    "            start_index = floor(((row.down_time - min) / dt_range) * NUMBER_OF_INTERVALS)\n",
    "\n",
    "            if start_index >= NUMBER_OF_INTERVALS:\n",
    "                start_index = NUMBER_OF_INTERVALS - 1\n",
    "\n",
    "            if start_index < 0:\n",
    "                start_index = 0\n",
    "            \n",
    "            match row.activity:\n",
    "                case \"Nonproduction\":\n",
    "                    x_train_1[index][2][start_index] += 1.0\n",
    "                case \"Input\" | \"Replace\":\n",
    "                    # Puncuation\n",
    "                    if row.down_event in PUNCUATION:\n",
    "                        x_train_1[index][6][start_index] += 1.0\n",
    "                    elif row.down_event == \"q\":\n",
    "                        x_train_1[index][7][start_index] += 1.0\n",
    "                    else:\n",
    "                        x_train_1[index][8][start_index] += 1.0\n",
    "\n",
    "                case \"Remove/Cut\":\n",
    "                    x_train_1[index][4][start_index] += 1.0\n",
    "                case \"Paste\":\n",
    "                    x_train_1[index][5][start_index] += 1.0\n",
    "                case _:\n",
    "                    if \"Move From\" in row.activity:\n",
    "                        x_train_1[index][3][start_index] += 1.0\n",
    "            \n",
    "            if x_train_1[index][0][start_index] < (row.word_count / WORD_MAX):\n",
    "                x_train_1[index][0][start_index] = (row.word_count / WORD_MAX)\n",
    "            \n",
    "            if x_train_1[index][1][start_index] < (row.cursor_position / MAX_CURSOR_POSITION):\n",
    "                x_train_1[index][1][start_index] = (row.cursor_position / MAX_CURSOR_POSITION)\n",
    "            \n",
    "            if x_train_1[index][9][start_index] < (row.event_id / EVENT_MAX):\n",
    "                x_train_1[index][9][start_index] = (row.event_id / EVENT_MAX)\n",
    "\n",
    "            if row.action_time < 66: # Training 0-25%\n",
    "                x_train_1[index][10][start_index] += 1.0\n",
    "            elif row.action_time < 93: # Training 25-50%\n",
    "                x_train_1[index][11][start_index] += 1.0\n",
    "            elif row.action_time < 122: # Training 50-75%\n",
    "                x_train_1[index][12][start_index] += 1.0\n",
    "            else: # 75%+\n",
    "                x_train_1[index][13][start_index] += 1.0\n",
    "\n",
    "        currentWordCount = 0\n",
    "        for i, wordCount in enumerate(np.nditer(x_train_1[index][0])):\n",
    "            if wordCount == 0:\n",
    "                x_train_1[index][0] = currentWordCount\n",
    "            elif currentWordCount != wordCount:\n",
    "                currentWordCount = wordCount\n",
    "\n",
    "        currentCursorPosition = 0\n",
    "        for i, cursor in enumerate(np.nditer(x_train_1[index][1])):\n",
    "            if cursor == 0:\n",
    "                x_train_1[index][1] = currentCursorPosition\n",
    "            elif cursor != currentCursorPosition:\n",
    "                currentCursorPosition = cursor\n",
    "\n",
    "        currentEventCount = 0\n",
    "        for i, event in enumerate(np.nditer(x_train_1[index][1])):\n",
    "            if event == 0:\n",
    "                x_train_1[index][9] = currentEventCount\n",
    "            elif event != currentEventCount:\n",
    "                currentEventCount = event\n",
    "                \n",
    "    while len(x_train_1) < minPositiveExamples:\n",
    "        randomExample1 = GenerateNewExample(x_train=x_train_1)\n",
    "        randomExample2 = GenerateNewExample(x_train=x_train_2)\n",
    "        x_train_1 = np.concatenate((x_train_1, [randomExample1]))\n",
    "        x_train_2 = np.concatenate((x_train_2, [randomExample2]))\n",
    "\n",
    "    x_train_1 = np.transpose(x_train_1, axes=(0, 2, 1))\n",
    "    return x_train_1, x_train_2\n",
    "\n",
    "def GenerateNewExample(x_train: np.array):\n",
    "    example_index = np.random.choice(len(x_train), 1)[0]\n",
    "    Noise = (0.005 * np.random.sample(x_train[0].shape)) - 0.0025\n",
    "    result = x_train[example_index] *  (1 + Noise)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingPrep(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_labels, score_counts = np.unique(train_scores.score, return_counts=True)\n",
    "\n",
    "train_x_1 = dict(zip(score_labels, [np.zeros((floor(count - (0.2*count)), NUM_FEATURES, int(NUMBER_OF_INTERVALS))) for count in score_counts]))\n",
    "test_x_1 = dict(zip(score_labels, [np.zeros((floor(0.2*count), NUM_FEATURES, int(NUMBER_OF_INTERVALS))) for count in score_counts]))\n",
    "\n",
    "train_x_2 = dict(zip(score_labels, [np.zeros((floor(count - (0.2*count)), NUM_FEATURES, int(NUMBER_OF_INTERVALS))) for count in score_counts]))\n",
    "test_x_2 = dict(zip(score_labels, [np.zeros((floor(0.2*count), NUM_FEATURES, int(NUMBER_OF_INTERVALS))) for count in score_counts]))\n",
    "\n",
    "\n",
    "for score_index, score in enumerate(score_labels):\n",
    "    count = score_counts[score_index]\n",
    "    ids = train_scores[train_scores[\"score\"] == (score)]\n",
    "    temp_train_x_1, temp_train_x_2 = DfToX(train_logs[train_logs[\"id\"].isin(ids.id)], minPositiveExamples=50)\n",
    "\n",
    "    cutoff = floor(count*0.2)\n",
    "    train_x_1[score] = temp_train_x_1[cutoff:]\n",
    "    test_x_1[score] = temp_train_x_1[:cutoff]\n",
    "\n",
    "    train_x_2[score] = temp_train_x_2[cutoff:]\n",
    "    test_x_2[score] = temp_train_x_2[:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE_INPUT_X1 = train_x_1[0.5].shape[1:]\n",
    "SHAPE_INPUT_X2 = train_x_2[0.5].shape[1:]\n",
    "\n",
    "print(SHAPE_INPUT_X1)\n",
    "print(SHAPE_INPUT_X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=160, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=8, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=512, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=2048, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=512, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=128, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=512, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=512, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=128, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=128, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "binary_threshold = 0.6\n",
    "loss_bin = [tf.keras.losses.binary_crossentropy]\n",
    "metrics_bin = [\n",
    "               tf.keras.metrics.BinaryAccuracy(threshold=binary_threshold), \n",
    "               tf.keras.metrics.FalsePositives(thresholds=binary_threshold), \n",
    "               tf.keras.metrics.FalseNegatives(thresholds=binary_threshold), \n",
    "               tf.keras.metrics.TruePositives(thresholds=binary_threshold), \n",
    "               tf.keras.metrics.TrueNegatives(thresholds=binary_threshold)\n",
    "            ]\n",
    "\n",
    "Split_1_Input, Split_2_Input, Split_Output = GenerateModelLayers()\n",
    "modelSplit = tf.keras.models.Model(inputs=[Split_1_Input, Split_2_Input], outputs=Split_Output, name=\"Split\")\n",
    "modelSplit.compile(optimizer='adam', loss=loss_bin, metrics=metrics_bin)\n",
    "modelSplit.summary()\n",
    "\n",
    "del modelSplit\n",
    "del Split_1_Input\n",
    "del Split_2_Input\n",
    "del Split_Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainScore(score: float, epochs: int = 10, num_models: int = 5, layerGenerator = GenerateModelLayers):\n",
    "    assert (score in train_x_1.keys())\n",
    "    assert (score in train_x_2.keys())\n",
    "    x1_train_pos = np.zeros((0, SHAPE_INPUT_X1[0], SHAPE_INPUT_X1[1]))\n",
    "    x2_train_pos = np.zeros((0, SHAPE_INPUT_X2[0]))\n",
    "    y_train_pos = np.zeros((0))\n",
    "\n",
    "    x1_train_neg = np.zeros((0, SHAPE_INPUT_X1[0], SHAPE_INPUT_X1[1]))\n",
    "    x2_train_neg = np.zeros((0, SHAPE_INPUT_X2[0]))\n",
    "    y_train_neg = np.zeros((0))\n",
    "\n",
    "    for key in train_x_1.keys():\n",
    "        if key == score:\n",
    "            x1_train_pos = np.concatenate([x1_train_pos, train_x_1[key]])\n",
    "            x2_train_pos = np.concatenate([x2_train_pos, train_x_2[key]])\n",
    "            y_train_pos = np.concatenate([y_train_pos, np.ones(len(train_x_1[key]))])\n",
    "        else:\n",
    "            x1_train_neg = np.concatenate([x1_train_neg, train_x_1[key]])\n",
    "            x2_train_neg = np.concatenate([x2_train_neg, train_x_2[key]])\n",
    "            y_train_neg = np.concatenate([y_train_neg, np.zeros(len(train_x_1[key]))])\n",
    "            \n",
    "    Split_1_Input, Split_2_Input, Split_Output = layerGenerator()\n",
    "    models = [tf.keras.models.Model(inputs=[Split_1_Input, Split_2_Input], outputs=Split_Output, name=f\"Score_{score}_{x}\") for x in range(num_models)]\n",
    "\n",
    "    x1_train_pos, x1_valid_pos, x2_train_pos, x2_valid_pos = train_test_split(x1_train_pos, x2_train_pos, test_size=0.25, shuffle = True)\n",
    "    x1_train_neg, x1_valid_neg, x2_train_neg, x2_valid_neg = train_test_split(x1_train_neg, x2_train_neg, test_size=0.25, shuffle = True)\n",
    "\n",
    "    y_train_pos = np.ones(len(x1_train_pos))\n",
    "    y_valid_pos = np.ones(len(x1_valid_pos))\n",
    "    y_train_neg = np.zeros(len(x1_train_neg))\n",
    "    y_valid_neg = np.zeros(len(x1_valid_neg))\n",
    "\n",
    "    positiveAmount = len(y_train_pos)\n",
    "    negativeAmount = len(y_train_neg)\n",
    "    amounts = (len(y_train_pos), len(y_valid_pos), len(y_train_neg), len(y_valid_neg))\n",
    "\n",
    "    x1_train = np.concatenate((x1_train_pos, x1_train_neg))\n",
    "    x1_valid = np.concatenate((x1_valid_pos, x1_valid_neg))\n",
    "    x2_train = np.concatenate((x2_train_pos, x2_train_neg))\n",
    "    x2_valid = np.concatenate((x2_valid_pos, x2_valid_neg))\n",
    "    y_train = np.concatenate((y_train_pos, y_train_neg))\n",
    "    y_valid = np.concatenate((y_valid_pos, y_valid_neg))\n",
    "\n",
    "    print(y_train.shape)\n",
    "    print(f\"Positive: {positiveAmount + len(y_valid_pos)}; Train: {positiveAmount}; Valid: {len(y_valid_pos)}\")\n",
    "    print(f\"Negative: {negativeAmount + len(y_valid_neg)}; Train: {negativeAmount}; Valid: {len(y_valid_neg)}\")\n",
    "    positiveWeight = (positiveAmount + negativeAmount) / (2 * positiveAmount)\n",
    "    negativeWeight = (positiveAmount + negativeAmount) / (2 * negativeAmount)\n",
    "\n",
    "    best_loss = 10000\n",
    "    best_model = None\n",
    "    best_history = None\n",
    "    for model in models:\n",
    "        print(f\"{model.name}\")\n",
    "        model.compile(optimizer='adam', loss=loss_bin, metrics=metrics_bin)\n",
    "        history = model.fit(x=[x1_train, x2_train], y=y_train, shuffle=True, validation_data=([x1_valid, x2_valid], y_valid), epochs=epochs, class_weight={0: negativeWeight, 1: positiveWeight})\n",
    "        if model.get_metrics_result()[\"loss\"] < best_loss:\n",
    "            best_loss = model.get_metrics_result()[\"loss\"]\n",
    "            best_model = model\n",
    "            best_history = history\n",
    "        print()\n",
    "    return best_model, best_history, amounts\n",
    "\n",
    "def DisplayHistory(history, epochs, amounts):\n",
    "    epochs_range = range(epochs)\n",
    "    rows = 3\n",
    "    cols = 2\n",
    "    displayIndex = 0\n",
    "\n",
    "    posTrainAmount = amounts[0] if amounts[0] != 0 else 1\n",
    "    posValidAmount = amounts[1] if amounts[1] != 0 else 1\n",
    "    negTrainAmount = amounts[2] if amounts[2] != 0 else 1\n",
    "    negValidAmount = amounts[3] if amounts[3] != 0 else 1\n",
    "\n",
    "    displayIndex += 1\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    fig = plt.figure(figsize=(16, 14))\n",
    "    #fig.suptitle(\"Training History\")\n",
    "    ax = fig.add_subplot(rows, cols, displayIndex)\n",
    "    ax.set_ylim(0, 3)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    displayIndex += 1\n",
    "    data = history.history['binary_accuracy']\n",
    "    val_data = history.history['val_binary_accuracy']\n",
    "    ax = fig.add_subplot(rows, cols, displayIndex)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.plot(epochs_range, data, label='Training')\n",
    "    plt.plot(epochs_range, val_data, label='Validation')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Binary Accuracy')\n",
    "\n",
    "    displayIndex += 1\n",
    "    data = history.history['true_positives']\n",
    "    val_data = history.history['val_true_positives']\n",
    "    ax = fig.add_subplot(rows, cols, displayIndex)\n",
    "    plt.plot(epochs_range, np.divide(data, posTrainAmount), label='Training')\n",
    "    plt.plot(epochs_range, np.divide(val_data, posValidAmount), label='Validation')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('True Positives')\n",
    "\n",
    "    displayIndex += 1\n",
    "    data = history.history['false_positives']\n",
    "    val_data = history.history['val_false_positives']\n",
    "    ax = fig.add_subplot(rows, cols, displayIndex)\n",
    "    plt.plot(epochs_range, np.divide(data, negTrainAmount), label='Training')\n",
    "    plt.plot(epochs_range, np.divide(val_data, negValidAmount), label='Validation')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('False Positives')\n",
    "\n",
    "    displayIndex += 1\n",
    "    data = history.history['true_negatives']\n",
    "    val_data = history.history['val_true_negatives']\n",
    "    ax = fig.add_subplot(rows, cols, displayIndex)\n",
    "    #ax.set_ylim(0, 3)\n",
    "    plt.plot(epochs_range, np.divide(data, negTrainAmount), label='Training')\n",
    "    plt.plot(epochs_range, np.divide(val_data, negValidAmount), label='Validation')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('True Negatives')\n",
    "\n",
    "    displayIndex += 1\n",
    "    data = history.history['false_negatives']\n",
    "    val_data = history.history['val_false_negatives']\n",
    "    ax = fig.add_subplot(rows, cols, displayIndex)\n",
    "    #ax.set_ylim(0, 3)\n",
    "    plt.plot(epochs_range, np.divide(data, posTrainAmount), label='Training')\n",
    "    plt.plot(epochs_range, np.divide(val_data, posValidAmount), label='Validation')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('False Negatives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers05():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=160, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    # layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "    #                                 kernel_size = (4,), \n",
    "    #                                 strides=1, \n",
    "    #                                 padding=\"SAME\", \n",
    "    #                                 activation='relu', \n",
    "    #                                 kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "    #                                 )(layers_1)\n",
    "    # layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    # layers_1 = tf.keras.layers.Conv1D(filters=8, \n",
    "    #                                 kernel_size = (4,), \n",
    "    #                                 strides=1, \n",
    "    #                                 padding=\"SAME\", \n",
    "    #                                 activation='relu', \n",
    "    #                                 kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "    #                                 )(layers_1)\n",
    "    # layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=512, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=128, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "epochs = 25\n",
    "model05, history, amounts = TrainScore(0.5, epochs=epochs, layerGenerator=GenerateModelLayers05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers10():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=160, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=512, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=128, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "epochs = 20\n",
    "model10, history, amounts = TrainScore(1.0, epochs=epochs, layerGenerator=GenerateModelLayers10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers15():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=160, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=24, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=256, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=256, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=256, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=128, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=128, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "epochs = 20\n",
    "model15, history, amounts = TrainScore(1.5, epochs=epochs, layerGenerator=GenerateModelLayers15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers20():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=160, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=512, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=128, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "epochs = 25\n",
    "model20, history, amounts = TrainScore(2.0, epochs=epochs, layerGenerator=GenerateModelLayers20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers25():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=160, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=6, \n",
    "                                    kernel_size = (2,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=512, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=256, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=256, activation='relu')(layers_2)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_2 = tf.keras.layers.Dense(units=64, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "epochs = 20\n",
    "model25, history, amounts = TrainScore(2.5, epochs=epochs, layerGenerator=GenerateModelLayers25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers30():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=160, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=8, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=512, activation='relu')(layers_1)\n",
    "    # layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    # layers_1 = tf.keras.layers.Dense(units=2048, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=1024, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=512, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=256, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=128, activation='relu')(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=96, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "epochs = 20\n",
    "model30, history, amounts = TrainScore(3.0, epochs=epochs, layerGenerator= GenerateModelLayers30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers35():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=160, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=64, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=512, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=128, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=64, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=128, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=72, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=36, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "model35, history, amounts = TrainScore(3.5, epochs=epochs, layerGenerator=GenerateModelLayers35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers40():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=80, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=32, \n",
    "                                    kernel_size = (2,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=24, \n",
    "                                    kernel_size = (2,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=256, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=64, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=256, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=144, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=64, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=128, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=36, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "epochs = 50\n",
    "model40, history, amounts = TrainScore(4.0, epochs=epochs, layerGenerator=GenerateModelLayers40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers45():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=80, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=32, \n",
    "                                    kernel_size = (2,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=24, \n",
    "                                    kernel_size = (2,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=256, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=64, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=256, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=144, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=64, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=128, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=36, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "epochs = 25\n",
    "model45, history, amounts = TrainScore(4.5, epochs=epochs, layerGenerator=GenerateModelLayers45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers50():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=80, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=32, \n",
    "                                    kernel_size = (2,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=24, \n",
    "                                    kernel_size = (2,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=256, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=64, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=256, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=64, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=128, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=64, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=36, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "epochs = 20\n",
    "model50, history, amounts = TrainScore(5.0, epochs=epochs, num_models=3, layerGenerator=GenerateModelLayers50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers55():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=80, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=32, \n",
    "                                    kernel_size = (2,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=24, \n",
    "                                    kernel_size = (2,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=256, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=64, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=256, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=144, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=64, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=128, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=36, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "epochs = 25\n",
    "model55, history, amounts = TrainScore(5.5, epochs=epochs, layerGenerator=GenerateModelLayers55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayers60():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=160, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=40, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=8, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=512, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=128, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=24, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=128, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=12, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output]))\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=80, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(layers_3)\n",
    "    return model_1.input, model_2.input, layers_Output\n",
    "\n",
    "epochs = 40\n",
    "model60, history, amounts = TrainScore(6.0, epochs=epochs, layerGenerator = GenerateModelLayers60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayHistory(history, epochs, amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllModels = {\n",
    "    0.5: model05,\n",
    "    1.0: model10,\n",
    "    1.5: model15,\n",
    "    2.0: model20,\n",
    "    2.5: model25,\n",
    "    3.0: model30,\n",
    "    3.5: model35,\n",
    "    4.0: model40,\n",
    "    4.5: model45,\n",
    "    5.0: model50,\n",
    "    5.5: model55,\n",
    "    6.0: model60\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TRAIN = 0\n",
    "\n",
    "for score_index, score in enumerate(score_labels):\n",
    "    TOTAL_TRAIN += train_x_1[score].shape[0]\n",
    "\n",
    "SHAPE_INPUT_XP = (TOTAL_TRAIN, 12)\n",
    "\n",
    "com_train_x_1 = np.zeros(shape=(TOTAL_TRAIN, train_x_1[0.5].shape[1], train_x_1[0.5].shape[2]))\n",
    "com_train_x_2 = np.zeros(shape=(TOTAL_TRAIN, train_x_2[0.5].shape[1]))\n",
    "com_train_x_P = np.zeros(shape=SHAPE_INPUT_XP)\n",
    "com_train_y = np.zeros(shape=(TOTAL_TRAIN))\n",
    "\n",
    "\n",
    "com_index = 0\n",
    "for score_index, score in enumerate(score_labels):\n",
    "    print(score)\n",
    "    for index, x in enumerate(train_x_1[score]):\n",
    "        com_train_x_1[com_index] = train_x_1[score][index]\n",
    "        com_train_x_2[com_index] = train_x_2[score][index]\n",
    "        com_train_y[com_index] = score\n",
    "\n",
    "        for score_index_2, score_2 in enumerate(score_labels):\n",
    "            prediction = AllModels[score_2].predict(\n",
    "                x=[np.array([train_x_1[score][index]]), np.array([train_x_2[score][index]])],\n",
    "                verbose=0\n",
    "                )\n",
    "            com_train_x_P[com_index][score_index_2] = prediction\n",
    "        \n",
    "        com_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Com_Train_x1: Shape - {com_train_x_1.shape}, Ex: {com_train_x_1[0]}\")\n",
    "print(f\"Com_Train_x2: Shape - {com_train_x_2.shape}, Ex: {com_train_x_2[0]}\")\n",
    "print(f\"Com_Train_xP: Shape - {com_train_x_P.shape}, Ex: {com_train_x_P[0]}\")\n",
    "print(f\"Com_Train_y:  Shape - {com_train_y.shape}, Ex: {com_train_y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModelLayersFinalModel():\n",
    "    input_shape_1 = SHAPE_INPUT_X1\n",
    "    input_shape_2 = SHAPE_INPUT_X2\n",
    "    input_shape_3 = (12,)\n",
    "\n",
    "    input_1 = tf.keras.Input(shape=input_shape_1, dtype=np.float64)\n",
    "    input_2 = tf.keras.Input(shape=input_shape_2, dtype=np.float64)\n",
    "    input_3 = tf.keras.Input(shape=input_shape_3, dtype=np.float64)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Normalization()(input_1)\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=80, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=2, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv1D(filters=8, \n",
    "                                    kernel_size = (4,), \n",
    "                                    strides=1, \n",
    "                                    padding=\"SAME\", \n",
    "                                    activation='relu', \n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "                                    )(layers_1)\n",
    "    layers_1 = tf.keras.layers.MaxPool1D()(layers_1)\n",
    "\n",
    "    layers_1 = tf.keras.layers.Flatten()(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.4)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=512, activation='relu')(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dropout(0.2)(layers_1)\n",
    "    layers_1 = tf.keras.layers.Dense(units=64, activation='relu')(layers_1)\n",
    "    model_1 = tf.keras.Model(inputs = input_1, outputs=layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Normalization()(input_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=NUM_FEATURES_2, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=512, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.4)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=1024, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dense(units=64, activation='relu')(layers_2)\n",
    "    layers_2 = tf.keras.layers.Dropout(0.2)(layers_2)\n",
    "    model_2 = tf.keras.models.Model(inputs = input_2, outputs=layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Flatten()(input_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=256, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=128, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.4)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dropout(0.2)(layers_3)\n",
    "    layers_3 = tf.keras.layers.Dense(units=12, activation='relu')(layers_3)\n",
    "    model_3 = tf.keras.models.Model(inputs = input_3, outputs=layers_3)\n",
    "\n",
    "    layers_4 = tf.keras.layers.Dense(units=16, activation='relu')(tf.keras.layers.concatenate([model_1.output, model_2.output, model_3.output]))\n",
    "    layers_4 = tf.keras.layers.Dropout(0.4)(layers_4)\n",
    "    layers_4 = tf.keras.layers.Dense(units=16, activation='relu')(layers_4)\n",
    "    layers_4 = tf.keras.layers.Dropout(0.4)(layers_4)\n",
    "    layers_Output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.linear)(layers_4)\n",
    "    return model_1.input, model_2.input, model_3.input, layers_Output\n",
    "\n",
    "loss_final = [tf.keras.losses.MeanAbsoluteError()]\n",
    "metrics_final = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "Split_1_Input, Split_2_Input, Split_3_Input, Split_Output = GenerateModelLayersFinalModel()\n",
    "modelCom = tf.keras.models.Model(inputs=[Split_1_Input, Split_2_Input, Split_3_Input], outputs=Split_Output, name=\"Final\")\n",
    "modelCom.compile(optimizer='adam', loss=loss_final, metrics=metrics_final)\n",
    "modelCom.summary()\n",
    "\n",
    "del modelCom\n",
    "del Split_1_Input\n",
    "del Split_2_Input\n",
    "del Split_3_Input\n",
    "del Split_Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 1\n",
    "epochs = 10\n",
    "Final_1_Input, Final_2_Input, Final_3_Input, Final_Output = GenerateModelLayersFinalModel()\n",
    "FinalModels = [tf.keras.models.Model(inputs=[Final_1_Input, Final_2_Input, Final_3_Input], outputs=Final_Output, name=f\"Final_Model_{x}\") for x in range(num_models)]\n",
    "\n",
    "best_loss_final = 10000\n",
    "best_model_final = None\n",
    "best_history_final = None\n",
    "\n",
    "for model in FinalModels:\n",
    "    print(f\"{model.name}\")\n",
    "    model.compile(optimizer='adam', loss=loss_final, metrics=metrics_final)\n",
    "    history_final = model.fit(x=[com_train_x_1, com_train_x_2, com_train_x_P], y=com_train_y, shuffle=True, validation_split=0.25, epochs=epochs)\n",
    "    if model.get_metrics_result()[\"loss\"] < best_loss_final:\n",
    "        best_loss_final = model.get_metrics_result()[\"loss\"]\n",
    "        best_model_final = model\n",
    "        best_history_final = history_final\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(epochs)\n",
    "rows = 1\n",
    "cols = 2\n",
    "\n",
    "loss = best_history_final.history['loss']\n",
    "val_loss = best_history_final.history['val_loss']\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "fig.suptitle(\"Best Final Training History\")\n",
    "ax = fig.add_subplot(rows, cols, 1)\n",
    "ax.set_ylim(0, 3)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "data = best_history_final.history['root_mean_squared_error']\n",
    "val_data = best_history_final.history['val_root_mean_squared_error']\n",
    "ax = fig.add_subplot(rows, cols, 2)\n",
    "#ax.set_ylim(0, 3)\n",
    "plt.plot(epochs_range, data, label='Training')\n",
    "plt.plot(epochs_range, val_data, label='Validation')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TEST = 0\n",
    "\n",
    "for score_index, score in enumerate(score_labels):\n",
    "    TOTAL_TEST += test_x_1[score].shape[0]\n",
    "\n",
    "# com_test_x_1 = np.zeros(shape=(TOTAL_TEST, test_x_1[0.5].shape[1], test_x_1[0.5].shape[2]))\n",
    "# com_test_x_2 = np.zeros(shape=(TOTAL_TEST, test_x_2[0.5].shape[1]))\n",
    "com_test_x_P = np.zeros(shape=(TOTAL_TEST, 12))\n",
    "com_test_y = np.zeros(shape=(TOTAL_TEST))\n",
    "com_test_pred = np.zeros(shape=(TOTAL_TEST))\n",
    "\n",
    "com_index = 0\n",
    "for score_index, score in enumerate(score_labels):\n",
    "    print(score)\n",
    "    for index, x in enumerate(test_x_1[score]):\n",
    "        # com_test_x_1[com_index] = test_x_1[score][index]\n",
    "        # com_test_x_2[com_index] = test_x_2[score][index]\n",
    "        individual_predictions = np.zeros(12)\n",
    "        com_test_y[com_index] = score\n",
    "\n",
    "        for score_index_2, score_2 in enumerate(score_labels):\n",
    "            prediction = AllModels[score_2].predict(x=[np.array([test_x_1[score][index]]), np.array([test_x_2[score][index]])], verbose=0)\n",
    "            individual_predictions[score_index_2] = prediction\n",
    "        \n",
    "        pred = best_model_final.predict(x=[\n",
    "            np.array([test_x_1[score][index]]), \n",
    "            np.array([test_x_2[score][index]]), \n",
    "            np.array([individual_predictions])\n",
    "            ],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        com_test_pred[com_index] = pred\n",
    "        com_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(com_test_pred - com_test_y)\n",
    "np.sqrt(np.mean((com_test_pred-com_test_y)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c75e15a --> 4.0\n",
    "# 2c7997a3 --> 3.5\n",
    "# 2d299968 --> 5.0\n",
    "# test_logs = pd.read_csv(\"./data/personaltest.csv\")\n",
    "\n",
    "predictions = []\n",
    "x_pred, x2_pred = DfToX(test_logs)\n",
    "BinaryScorePredictions = np.array([\n",
    "    AllModels[0.5].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[1.0].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[1.5].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[2.0].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[2.5].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[3.0].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[3.5].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[4.0].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[4.5].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[5.0].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[5.5].predict(x=[x_pred, x2_pred], verbose=0),\n",
    "    AllModels[6.0].predict(x=[x_pred, x2_pred], verbose=0)\n",
    "])\n",
    "BinaryScorePredictions = np.transpose(BinaryScorePredictions, axes=(1, 0, 2))\n",
    "\n",
    "print(x_pred.shape)\n",
    "print(x2_pred.shape)\n",
    "print(BinaryScorePredictions.shape)\n",
    "print(BinaryScorePredictions)\n",
    "\n",
    "predictions_final = best_model_final.predict(x=[x_pred, x2_pred, BinaryScorePredictions], verbose=0)\n",
    "predictions = []\n",
    "for index, id in enumerate(test_logs[\"id\"].unique()):\n",
    "    pred_score = 3.5\n",
    "    try:\n",
    "        pred_score = predictions_final[index][0]\n",
    "        print(pred_score)\n",
    "    except ValueError as ex:\n",
    "        pred_score = 3.5\n",
    "        print(f\"VALUE ERROR OCCURED. ERROR: {ex}\")\n",
    "    except Exception as ex:\n",
    "        print(f\"ERROR: {ex}\")\n",
    "        pred_score = 3.5\n",
    "    finally:\n",
    "        predictions.append((id, pred_score))\n",
    "\n",
    "ids = [p[0] for p in predictions]\n",
    "scores = [p[1] for p in predictions]\n",
    "\n",
    "sample_submission[\"id\"] = ids\n",
    "sample_submission[\"score\"] = scores\n",
    "sample_submission[[\"id\", \"score\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totaltime2 = time.time()\n",
    "print(totaltime2 - totaltime1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((totaltime2-totaltime1)/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
