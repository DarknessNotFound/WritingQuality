{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-14T20:26:21.594173Z","iopub.status.busy":"2023-10-14T20:26:21.593701Z","iopub.status.idle":"2023-10-14T20:26:31.310483Z","shell.execute_reply":"2023-10-14T20:26:31.309558Z","shell.execute_reply.started":"2023-10-14T20:26:21.594102Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","# import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","import tensorflow as tf\n","import numpy as np\n","from sklearn import preprocessing\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","from os.path import exists as DoesPathExist\n","train_logs_path = \"/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\"\n","train_scores_path = \"/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\"\n","test_logs_path = \"/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\"\n","sample_submission_path = \"/kaggle/input/linking-writing-processes-to-writing-quality/sample_submission.csv\"\n","\n","if DoesPathExist(train_logs_path):\n","    train_logs = pd.read_csv(train_logs_path)\n","else:\n","    train_logs = pd.read_csv(\"./data/train_logs.csv\")\n","\n","if DoesPathExist(train_scores_path):\n","    train_scores = pd.read_csv(train_scores_path)\n","else:\n","    train_scores = pd.read_csv(\"./data/train_scores.csv\")\n","\n","if DoesPathExist(test_logs_path):\n","    test_logs = pd.read_csv(test_logs_path)\n","else:\n","    test_logs = pd.read_csv(\"./data/test_logs.csv\")\n","    \n","if DoesPathExist(sample_submission_path):\n","    sample_submission = pd.read_csv(sample_submission_path)\n","else:\n","    sample_submission = pd.read_csv(\"./data/sample_submission.csv\")\n","    \n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","from IPython.display import display, HTML\n","display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n","pd.set_option('expand_frame_repr', False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T20:26:31.312949Z","iopub.status.busy":"2023-10-14T20:26:31.312354Z","iopub.status.idle":"2023-10-14T20:26:31.325060Z","shell.execute_reply":"2023-10-14T20:26:31.323980Z","shell.execute_reply.started":"2023-10-14T20:26:31.312917Z"},"trusted":true},"outputs":[],"source":["train_logs.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T20:26:31.327039Z","iopub.status.busy":"2023-10-14T20:26:31.326736Z","iopub.status.idle":"2023-10-14T20:26:31.345408Z","shell.execute_reply":"2023-10-14T20:26:31.344442Z","shell.execute_reply.started":"2023-10-14T20:26:31.327013Z"},"trusted":true},"outputs":[],"source":["train_scores.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_logs.shape\n","\n","cols_to_drop = [\"down_event\", \"up_event\"]\n","for col in cols_to_drop:\n","    train_logs = train_logs.drop(col, axis=1) \n","train_logs.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_groups: pd.DataFrame.groupby = train_logs.groupby([\"id\"])\n","input_counts = train_groups.activity.count()\n","print(input_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def FeatureEngineering(df: pd.DataFrame):\n","    \"\"\"Computes the feature engineering for training.\n","\n","    Args:\n","        df (pd.DataFrame): Input dataframe.\n","\n","    Returns:\n","        pd.DataFrame: This will be prepped with everything needed to be inputed as x-values for a dataframe.\n","    \"\"\"\n","    groups = df.groupby(\"id\")\n","    tails = groups.tail(1)\n","    valueCounts = groups[\"activity\"].value_counts()\n","\n","    result = pd.DataFrame(\n","        {\n","            \"id\": groups[\"id\"],\n","            \"num_events\": tails[\"event_id\"],\n","            \"word_count\": tails[\"word_count\"],\n","           # \"insert_count\": insert_count,\n","           # \"Remove/Cut\" : removal_count\n","        }\n","    )\n","\n","\n","    return result\n","\n","train_engineered = FeatureEngineering(df=train_logs)\n","train_engineered.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def NormalizeTrainData(input: pd.DataFrame) -> pd.DataFrame:\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","    input[\"num_events\"] = min_max_scaler.fit_transform(input[[\"num_events\"]])\n","    input[\"word_count\"] = min_max_scaler.fit_transform(input[[\"word_count\"]])\n","    return input\n","\n","train_data = NormalizeTrainData(train_engineered)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def OneHotEncoding(n: float):\n","    result = [0]*12\n","    result[int(n / 0.5) - 1] = 1\n","    return result\n","\n","x_train = train_data.sort_values(\"id\").drop(\"id\", axis=1)\n","raw_scores = train_scores.sort_values(\"id\").drop(\"id\", axis=1)\n","y_train = np.array([OneHotEncoding(float(y)) for y in raw_scores['score']])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_train = len(y_train)\n","x_valid = x_train[:int(num_train*0.2)]\n","y_valid = y_train[:int(num_train*0.2)]\n","x_train = x_train[int(num_train*0.2):]\n","y_train = y_train[int(num_train*0.2):]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = Sequential([\n","    Dense(units=512, activation='relu'),\n","    Dense(units=512, activation='relu'),\n","    Dense(units=512, activation='relu'),\n","    Dense(units=12, activation=\"sigmoid\")\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = model(np.array(x_train)).numpy()\n","predictions\n","tf.nn.softmax(predictions).numpy()\n","loss_fn = tf.keras.losses.CategoricalCrossentropy()\n","loss_fn(y_train, predictions).numpy()\n","optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)\n","model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n","model.fit(x_train, y_train, epochs=50, verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = model.evaluate(x_valid, y_valid)\n","for a in range(len(results)):\n","    print(f\"{model.metrics_names[a]}: {results[a]:.3f}\")\n","\n","predictions: np.array = model.predict(x_valid)\n","scaled_Pred = []\n","for p in predictions:\n","    max = p.max()\n","    scaled_Pred.append(np.array([1 if x == max else 0 for x in p]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Generate the submission\n","test_data = FeatureEngineering(test_logs)\n","test_data = NormalizeTrainData(test_data)\n","predictions = []\n","for index, row in test_data.iterrows():\n","    x_data = [x for x in row[1:]]\n","    pred = model.predict([x_data])\n","    max = pred[0].max()\n","    pred = [1 if x == max else 0 for x in pred[0]]\n","    result = 0.5\n","    for index, p in enumerate(pred):\n","        if p == 1:\n","            result = 0.5 * index + 0.5\n","    \n","    predictions.append((row[0], result)) # (Id, result)\n","print(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ids = [p[0] for p in predictions]\n","scores = [p[1] for p in predictions]\n","# ids = [1, 2, 3]\n","# scores = [0.5, 3.0, 4.5]\n","sample_submission[\"id\"] = ids\n","sample_submission[\"score\"] = scores\n","sample_submission[[\"id\", \"score\"]].to_csv(\"submission.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":4}
